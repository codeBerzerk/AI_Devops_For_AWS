# ðŸ—ï¸ K8s LLM Admin - ÐŸÐ¾Ð²Ð½Ð° ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Python Ð¿Ñ€Ð¾ÐµÐºÑ‚Ñƒ

## ðŸ“ Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Ñ„Ð°Ð¹Ð»Ñ–Ð²

```
k8s-llm-admin/
â”œâ”€â”€ README.md                          # Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ñ–Ñ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ñƒ
â”œâ”€â”€ requirements.txt                   # Python Ð·Ð°Ð»ÐµÐ¶Ð½Ð¾ÑÑ‚Ñ–
â”œâ”€â”€ setup.py                          # Package setup
â”œâ”€â”€ .env.example                      # ÐŸÑ€Ð¸ÐºÐ»Ð°Ð´ environment variables
â”œâ”€â”€ .gitignore                        # Git ignore file
â”‚
â”œâ”€â”€ config/                           # ÐšÐ¾Ð½Ñ„Ñ–Ð³ÑƒÑ€Ð°Ñ†Ñ–Ñ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py                   # Ð“Ð¾Ð»Ð¾Ð²Ð½Ñ– Ð½Ð°Ð»Ð°ÑˆÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ
â”‚   â””â”€â”€ logging_config.py             # Logging setup
â”‚
â”œâ”€â”€ prompts/                          # Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñ–Ð²
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ system_prompts.py             # Ð’Ð–Ð• Ð¡Ð¢Ð’ÐžÐ Ð•ÐÐ˜Ð™ â˜‘ï¸
â”‚   â”œâ”€â”€ templates.py                  # Jinja2 templates Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñ–Ð²
â”‚   â””â”€â”€ validators.py                 # Ð’Ð°Ð»Ñ–Ð´Ð°Ñ†Ñ–Ñ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñ–Ð²
â”‚
â”œâ”€â”€ llm/                              # LLM Ð²Ð·Ð°Ñ”Ð¼Ð¾Ð´Ñ–Ñ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ollama_client.py              # Ollama API client
â”‚   â”œâ”€â”€ prompt_manager.py             # Ð£Ð¿Ñ€Ð°Ð²Ð»Ñ–Ð½Ð½Ñ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°Ð¼Ð¸
â”‚   â”œâ”€â”€ response_parser.py            # ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÐµÐ¹ LLM
â”‚   â””â”€â”€ cache.py                      # ÐšÐµÑˆÑƒÐ²Ð°Ð½Ð½Ñ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÐµÐ¹
â”‚
â”œâ”€â”€ k8s/                              # Kubernetes Ñ–Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ñ–Ñ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ kubectl_wrapper.py            # ÐžÐ±Ð³Ð¾Ñ€Ñ‚ÐºÐ° Ð´Ð»Ñ kubectl
â”‚   â”œâ”€â”€ resource_inspector.py         # Ð†Ð½ÑÐ¿ÐµÐºÑ†Ñ–Ñ Ñ€ÐµÑÑƒÑ€ÑÑ–Ð²
â”‚   â””â”€â”€ cluster_info.py               # Ð†Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–Ñ Ð¿Ñ€Ð¾ ÐºÐ»Ð°ÑÑ‚ÐµÑ€
â”‚
â”œâ”€â”€ rag/                              # RAG ÑÐ¸ÑÑ‚ÐµÐ¼Ð° (ÐšÑ€Ð¾Ðº 2)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ vector_store.py               # Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ð° Ð‘Ð”
â”‚   â”œâ”€â”€ embeddings.py                 # Embedding Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ñ–Ñ
â”‚   â”œâ”€â”€ retriever.py                  # ÐŸÐ¾ÑˆÑƒÐº Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ–Ð²
â”‚   â””â”€â”€ knowledge_base/               # Ð‘Ð°Ð·Ð° Ð·Ð½Ð°Ð½ÑŒ
â”‚       â”œâ”€â”€ k8s_docs/                 # K8s Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ñ–Ñ
â”‚       â”œâ”€â”€ runbooks/                 # Runbooks
â”‚       â””â”€â”€ incidents/                # Ð†ÑÑ‚Ð¾Ñ€Ñ–Ñ Ñ–Ð½Ñ†Ð¸Ð´ÐµÐ½Ñ‚Ñ–Ð²
â”‚
â”œâ”€â”€ api/                              # Backend API (ÐšÑ€Ð¾Ðº 3)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                       # FastAPI app
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ diagnose.py               # Ð”Ñ–Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ° endpoints
â”‚   â”‚   â”œâ”€â”€ kubectl.py                # kubectl ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¸
â”‚   â”‚   â””â”€â”€ health.py                 # Health checks
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ request.py                # Request schemas
â”‚   â”‚   â””â”€â”€ response.py               # Response schemas
â”‚   â””â”€â”€ middleware/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ auth.py                   # ÐÑƒÑ‚ÐµÐ½Ñ‚Ð¸Ñ„Ñ–ÐºÐ°Ñ†Ñ–Ñ
â”‚       â””â”€â”€ rate_limit.py             # Rate limiting
â”‚
â”œâ”€â”€ utils/                            # Ð£Ñ‚Ð¸Ð»Ñ–Ñ‚Ð¸
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py                     # Logging helper
â”‚   â”œâ”€â”€ metrics.py                    # ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸
â”‚   â””â”€â”€ validators.py                 # Ð’Ð°Ð»Ñ–Ð´Ð°Ñ†Ñ–Ñ input
â”‚
â”œâ”€â”€ tests/                            # Ð¢ÐµÑÑ‚Ð¸
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_prompts.py               # Ð¢ÐµÑÑ‚Ð¸ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñ–Ð²
â”‚   â”œâ”€â”€ test_llm.py                   # Ð¢ÐµÑÑ‚Ð¸ LLM
â”‚   â”œâ”€â”€ test_k8s.py                   # Ð¢ÐµÑÑ‚Ð¸ K8s
â”‚   â””â”€â”€ test_api.py                   # Ð¢ÐµÑÑ‚Ð¸ API
â”‚
â”œâ”€â”€ scripts/                          # Ð”Ð¾Ð¿Ð¾Ð¼Ñ–Ð¶Ð½Ñ– ÑÐºÑ€Ð¸Ð¿Ñ‚Ð¸
â”‚   â”œâ”€â”€ setup_jetson.sh               # ÐÐ°Ð»Ð°ÑˆÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ Jetson
â”‚   â”œâ”€â”€ download_models.sh            # Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
â”‚   â””â”€â”€ benchmark.py                  # Benchmarking
â”‚
â”œâ”€â”€ docs/                             # Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ñ–Ñ
â”‚   â”œâ”€â”€ api.md                        # API Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ñ–Ñ
â”‚   â”œâ”€â”€ prompts.md                    # Prompt engineering guide
â”‚   â”œâ”€â”€ deployment.md                 # Deployment guide
â”‚   â””â”€â”€ examples/                     # ÐŸÑ€Ð¸ÐºÐ»Ð°Ð´Ð¸ Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð½Ñ
â”‚
â””â”€â”€ examples/                         # ÐŸÑ€Ð¸ÐºÐ»Ð°Ð´Ð¸ ÐºÐ¾Ð´Ñƒ
    â”œâ”€â”€ simple_diagnostic.py
    â”œâ”€â”€ batch_analysis.py
    â””â”€â”€ streaming_response.py
```

---

## ðŸ“ Ð”ÐµÑ‚Ð°Ð»ÑŒÐ½Ð¸Ð¹ Ð¾Ð¿Ð¸Ñ ÐºÐ¾Ð¶Ð½Ð¾Ð³Ð¾ Ñ„Ð°Ð¹Ð»Ñƒ

### 1. **prompts/system_prompts.py** âœ… Ð’Ð–Ð• Ð“ÐžÐ¢ÐžÐ’Ðž

Ð”Ð¸Ð². Ð¿Ð¾Ð¿ÐµÑ€ÐµÐ´Ð½Ñ–Ð¹ artifact - Ð¼Ñ–ÑÑ‚Ð¸Ñ‚ÑŒ:
- Ð‘Ð°Ð·Ð¾Ð²Ñ– system prompts
- Ð¡Ð¿ÐµÑ†Ñ–Ð°Ð»Ñ–Ð·Ð¾Ð²Ð°Ð½Ñ– Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð¸ (Pod, Network, Node, Deployment, Performance)
- Ð”Ð¸Ð½Ð°Ð¼Ñ–Ñ‡Ð½Ð° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ñ–Ñ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñ–Ð²
- Few-shot examples
- Safety prompts
- Prompt optimization utilities

---

### 2. **prompts/templates.py**

```python
"""
Jinja2 templates Ð´Ð»Ñ Ð´Ð¸Ð½Ð°Ð¼Ñ–Ñ‡Ð½Ð¾Ñ— Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ñ–Ñ— Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñ–Ð²
Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ñ‚ÑŒÑÑ Ð´Ð»Ñ Ð±Ñ–Ð»ÑŒÑˆ Ð³Ð½ÑƒÑ‡ÐºÐ¾Ð³Ð¾ template rendering
"""

from jinja2 import Environment, BaseLoader, Template
from typing import Dict, Any


# Template Ð´Ð»Ñ Ð±Ð°Ð·Ð¾Ð²Ð¾Ñ— Ð´Ñ–Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ¸
DIAGNOSTIC_TEMPLATE = """
{% if severity == "critical" %}
ðŸš¨ CRITICAL INCIDENT - IMMEDIATE ACTION REQUIRED
{% elif severity == "high" %}
âš ï¸ HIGH PRIORITY ISSUE
{% endif %}

# Kubernetes Issue Report

**Resource Type:** {{ resource_type }}
**Namespace:** {{ namespace }}
**Cluster:** {{ cluster_name }} ({{ k8s_version }})
**Reported:** {{ timestamp }}

## User Description:
{{ issue_description }}

{% if kubectl_output %}
## Available Diagnostic Data:
```
{{ kubectl_output }}
```
{% endif %}

{% if recent_changes %}
## Recent Changes Detected:
{% for change in recent_changes %}
- {{ change.timestamp }}: {{ change.description }}
{% endfor %}
{% endif %}

{% if similar_incidents %}
## Similar Past Incidents:
{% for incident in similar_incidents %}
- [{{ incident.date }}] {{ incident.title }} - {{ incident.resolution_summary }}
{% endfor %}
{% endif %}

Now, provide a structured diagnostic response.
"""


# Template Ð´Ð»Ñ follow-up Ð¿Ð¸Ñ‚Ð°Ð½ÑŒ
FOLLOWUP_TEMPLATE = """
# Conversation Context

## Previous Diagnosis:
{{ previous_diagnosis }}

## Actions Taken:
{% for action in actions_taken %}
{{ loop.index }}. {{ action.description }}
   Command: `{{ action.command }}`
   Result: {{ action.result }}
{% endfor %}

## Current Situation:
{{ current_status }}

## New Information:
{{ new_info }}

## User Follow-up Question:
{{ user_question }}

Given this context, provide updated guidance. Don't repeat what's already been covered.
"""


# Template Ð´Ð»Ñ multi-resource Ð°Ð½Ð°Ð»Ñ–Ð·Ñƒ
MULTI_RESOURCE_TEMPLATE = """
# Multi-Resource Analysis Required

You are analyzing multiple Kubernetes resources together to diagnose a system-wide issue.

## Resources Involved:
{% for resource in resources %}
### {{ resource.type }}: {{ resource.name }}
**Status:** {{ resource.status }}
**Last Update:** {{ resource.last_update }}

{% if resource.logs %}
**Recent Logs:**
```
{{ resource.logs | truncate(500) }}
```
{% endif %}

{% if resource.events %}
**Events:**
{% for event in resource.events[-5:] %}
- {{ event.timestamp }}: {{ event.message }}
{% endfor %}
{% endif %}

---
{% endfor %}

## System-Wide Symptoms:
{{ system_symptoms }}

Analyze these resources holistically and identify cascading failures or root cause.
"""


class PromptTemplateManager:
    """Ð£Ð¿Ñ€Ð°Ð²Ð»Ñ–Ð½Ð½Ñ Jinja2 templates Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñ–Ð²"""
    
    def __init__(self):
        self.env = Environment(loader=BaseLoader())
        self.env.filters['truncate'] = lambda s, length: s[:length] + '...' if len(s) > length else s
    
    def render_diagnostic(self, context: Dict[str, Any]) -> str:
        """Ð ÐµÐ½Ð´ÐµÑ€ Ð´Ñ–Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸Ñ‡Ð½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°"""
        template = self.env.from_string(DIAGNOSTIC_TEMPLATE)
        return template.render(**context)
    
    def render_followup(self, context: Dict[str, Any]) -> str:
        """Ð ÐµÐ½Ð´ÐµÑ€ follow-up Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°"""
        template = self.env.from_string(FOLLOWUP_TEMPLATE)
        return template.render(**context)
    
    def render_multi_resource(self, context: Dict[str, Any]) -> str:
        """Ð ÐµÐ½Ð´ÐµÑ€ multi-resource Ð°Ð½Ð°Ð»Ñ–Ð·Ñƒ"""
        template = self.env.from_string(MULTI_RESOURCE_TEMPLATE)
        return template.render(**context)
    
    def render_custom(self, template_str: str, context: Dict[str, Any]) -> str:
        """Ð ÐµÐ½Ð´ÐµÑ€ ÐºÐ°ÑÑ‚Ð¾Ð¼Ð½Ð¾Ð³Ð¾ template"""
        template = self.env.from_string(template_str)
        return template.render(**context)
```

**ÐŸÑ€Ð¸Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ:** Ð“Ð½ÑƒÑ‡ÐºÐµ ÑÑ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñ–Ð² Ð· Ð´Ð¸Ð½Ð°Ð¼Ñ–Ñ‡Ð½Ð¸Ð¼Ð¸ Ð´Ð°Ð½Ð¸Ð¼Ð¸ Ñ‡ÐµÑ€ÐµÐ· Jinja2 templates.

---

### 3. **prompts/validators.py**

```python
"""
Ð’Ð°Ð»Ñ–Ð´Ð°Ñ†Ñ–Ñ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñ–Ð² Ð¿ÐµÑ€ÐµÐ´ Ð²Ñ–Ð´Ð¿Ñ€Ð°Ð²ÐºÐ¾ÑŽ Ð´Ð¾ LLM
ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð±ÐµÐ·Ð¿ÐµÐºÐ¸, Ð´Ð¾Ð²Ð¶Ð¸Ð½Ð¸, Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ÑƒÐ²Ð°Ð½Ð½Ñ
"""

import re
from typing import List, Tuple, Optional
from enum import Enum


class ValidationError(Exception):
    """ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð²Ð°Ð»Ñ–Ð´Ð°Ñ†Ñ–Ñ— Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°"""
    pass


class SecurityLevel(Enum):
    """Ð Ñ–Ð²Ð½Ñ– Ð±ÐµÐ·Ð¿ÐµÐºÐ¸ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ñ–Ð¹"""
    SAFE = "safe"              # Ð¢Ñ–Ð»ÑŒÐºÐ¸ Ñ‡Ð¸Ñ‚Ð°Ð½Ð½Ñ
    MODERATE = "moderate"      # Ð—Ð¼Ñ–Ð½Ð¸ Ð· low impact
    DESTRUCTIVE = "destructive"  # Ð’Ð¸Ð´Ð°Ð»ÐµÐ½Ð½Ñ, scale to 0, etc.


class PromptValidator:
    """Ð’Ð°Ð»Ñ–Ð´Ð°Ñ†Ñ–Ñ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñ–Ð² Ð½Ð° Ð±ÐµÐ·Ð¿ÐµÐºÑƒ Ñ‚Ð° ÐºÐ¾Ñ€ÐµÐºÑ‚Ð½Ñ–ÑÑ‚ÑŒ"""
    
    # ÐÐµÐ±ÐµÐ·Ð¿ÐµÑ‡Ð½Ñ– kubectl ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¸
    DESTRUCTIVE_COMMANDS = [
        r'kubectl\s+delete',
        r'kubectl\s+drain',
        r'kubectl\s+cordon',
        r'kubectl\s+taint.*NoSchedule',
        r'kubectl\s+scale.*--replicas=0',
        r'kubectl\s+patch.*delete',
    ]
    
    # Ð—Ð°Ð±Ð¾Ñ€Ð¾Ð½ÐµÐ½Ñ– patterns Ð² Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°Ñ…
    FORBIDDEN_PATTERNS = [
        r'rm\s+-rf',
        r'sudo\s+rm',
        r'DROP\s+TABLE',
        r'DELETE\s+FROM.*WHERE\s+1=1',
        r'--force.*--grace-period=0',
    ]
    
    def __init__(self, max_prompt_length: int = 8000):
        self.max_length = max_prompt_length
    
    def validate_prompt(
        self, 
        prompt: str, 
        allow_destructive: bool = False
    ) -> Tuple[bool, Optional[str]]:
        """
        Ð’Ð°Ð»Ñ–Ð´Ð°Ñ†Ñ–Ñ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°
        
        Args:
            prompt: ÐŸÑ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ð¿ÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ¸
            allow_destructive: Ð§Ð¸ Ð´Ð¾Ð·Ð²Ð¾Ð»ÐµÐ½Ñ– Ð´ÐµÑÑ‚Ñ€ÑƒÐºÑ‚Ð¸Ð²Ð½Ñ– Ð¾Ð¿ÐµÑ€Ð°Ñ†Ñ–Ñ—
        
        Returns:
            (is_valid, error_message)
        """
        # 1. ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð´Ð¾Ð²Ð¶Ð¸Ð½Ð¸
        if len(prompt) > self.max_length:
            return False, f"Prompt too long: {len(prompt)} > {self.max_length}"
        
        # 2. ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð½Ð° Ð¿Ð¾Ñ€Ð¾Ð¶Ð½Ñ–Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚
        if not prompt.strip():
            return False, "Empty prompt"
        
        # 3. ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð½Ð° Ð·Ð°Ð±Ð¾Ñ€Ð¾Ð½ÐµÐ½Ñ– patterns
        for pattern in self.FORBIDDEN_PATTERNS:
            if re.search(pattern, prompt, re.IGNORECASE):
                return False, f"Forbidden pattern detected: {pattern}"
        
        # 4. ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð½Ð° Ð´ÐµÑÑ‚Ñ€ÑƒÐºÑ‚Ð¸Ð²Ð½Ñ– ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¸
        if not allow_destructive:
            for pattern in self.DESTRUCTIVE_COMMANDS:
                if re.search(pattern, prompt, re.IGNORECASE):
                    return False, f"Destructive command detected: {pattern}. Set allow_destructive=True if intentional."
        
        # 5. ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð½Ð° injection attacks
        if self._check_injection(prompt):
            return False, "Potential injection attack detected"
        
        return True, None
    
    def _check_injection(self, prompt: str) -> bool:
        """ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð½Ð° prompt injection ÑÐ¿Ñ€Ð¾Ð±Ð¸"""
        injection_indicators = [
            "ignore previous instructions",
            "disregard all prior",
            "new instructions:",
            "system: you are now",
            "forget everything above",
        ]
        
        prompt_lower = prompt.lower()
        return any(indicator in prompt_lower for indicator in injection_indicators)
    
    def classify_security_level(self, prompt: str) -> SecurityLevel:
        """ÐšÐ»Ð°ÑÐ¸Ñ„Ñ–ÐºÐ°Ñ†Ñ–Ñ Ñ€Ñ–Ð²Ð½Ñ Ð±ÐµÐ·Ð¿ÐµÐºÐ¸ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ñ–Ñ—"""
        # Ð”ÐµÑÑ‚Ñ€ÑƒÐºÑ‚Ð¸Ð²Ð½Ñ– Ð¾Ð¿ÐµÑ€Ð°Ñ†Ñ–Ñ—
        for pattern in self.DESTRUCTIVE_COMMANDS:
            if re.search(pattern, prompt, re.IGNORECASE):
                return SecurityLevel.DESTRUCTIVE
        
        # ÐšÐ¾Ð¼Ð°Ð½Ð´Ð¸ Ð·Ð¼Ñ–Ð½Ð¸ ÑÑ‚Ð°Ð½Ñƒ
        moderate_patterns = [
            r'kubectl\s+apply',
            r'kubectl\s+patch',
            r'kubectl\s+scale',
            r'kubectl\s+rollout\s+restart',
        ]
        
        for pattern in moderate_patterns:
            if re.search(pattern, prompt, re.IGNORECASE):
                return SecurityLevel.MODERATE
        
        # Ð‘ÐµÐ·Ð¿ÐµÑ‡Ð½Ñ– Ð¾Ð¿ÐµÑ€Ð°Ñ†Ñ–Ñ— (Ñ‚Ñ–Ð»ÑŒÐºÐ¸ Ñ‡Ð¸Ñ‚Ð°Ð½Ð½Ñ)
        return SecurityLevel.SAFE
    
    def extract_kubectl_commands(self, prompt: str) -> List[str]:
        """Ð’Ð¸Ñ‚ÑÐ³Ñ‚Ð¸ Ð²ÑÑ– kubectl ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¸ Ð· Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°"""
        # Regex Ð´Ð»Ñ kubectl ÐºÐ¾Ð¼Ð°Ð½Ð´
        pattern = r'kubectl\s+[^\n]+'
        commands = re.findall(pattern, prompt)
        return [cmd.strip() for cmd in commands]
    
    def sanitize_sensitive_data(self, prompt: str) -> str:
        """Ð’Ð¸Ð´Ð°Ð»ÐµÐ½Ð½Ñ Ñ‡ÑƒÑ‚Ð»Ð¸Ð²Ð¸Ñ… Ð´Ð°Ð½Ð¸Ñ… Ð· Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð° Ð¿ÐµÑ€ÐµÐ´ Ð»Ð¾Ð³ÑƒÐ²Ð°Ð½Ð½ÑÐ¼"""
        # ÐœÐ°ÑÐºÑƒÐ²Ð°Ð½Ð½Ñ ÑÐµÐºÑ€ÐµÑ‚Ñ–Ð²
        prompt = re.sub(
            r'(password|token|secret|key)[\s:=]+[^\s]+',
            r'\1=***REDACTED***',
            prompt,
            flags=re.IGNORECASE
        )
        
        # ÐœÐ°ÑÐºÑƒÐ²Ð°Ð½Ð½Ñ IP Ð°Ð´Ñ€ÐµÑ
        prompt = re.sub(
            r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b',
            'XXX.XXX.XXX.XXX',
            prompt
        )
        
        return prompt


# Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¸Ð¹ validator instance
validator = PromptValidator()
```

**ÐŸÑ€Ð¸Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ:** Ð‘ÐµÐ·Ð¿ÐµÐºÐ° - Ð¿ÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñ–Ð² Ð½Ð° Ð½ÐµÐ±ÐµÐ·Ð¿ÐµÑ‡Ð½Ñ– ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¸, injection Ð°Ñ‚Ð°ÐºÐ¸, Ð²Ð¸Ñ‚Ñ–Ðº Ñ‡ÑƒÑ‚Ð»Ð¸Ð²Ð¸Ñ… Ð´Ð°Ð½Ð¸Ñ….

---

### 4. **llm/ollama_client.py**

```python
"""
Client Ð´Ð»Ñ Ð²Ð·Ð°Ñ”Ð¼Ð¾Ð´Ñ–Ñ— Ð· Ollama API
ÐžÐ±Ð³Ð¾Ñ€Ñ‚ÐºÐ° Ð´Ð»Ñ HTTP Ð·Ð°Ð¿Ð¸Ñ‚Ñ–Ð² Ð· retry logic, streaming, caching
"""

import requests
import json
import time
from typing import Dict, Any, Optional, Generator, List
from dataclasses import dataclass
from enum import Enum

from config.settings import settings
from utils.logger import logger


class ModelSize(Enum):
    """Ð Ð¾Ð·Ð¼Ñ–Ñ€Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹"""
    SMALL = "1-2B"    # DeepSeek Coder 1.3B
    MEDIUM = "3-4B"   # Llama 3.2 3B, Phi-3-mini
    LARGE = "7B+"     # CodeLlama 7B


@dataclass
class LLMResponse:
    """Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¾Ð²Ð°Ð½Ð° Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ Ð²Ñ–Ð´ LLM"""
    text: str
    model: str
    tokens_generated: int
    generation_time: float
    prompt_tokens: int
    cached: bool = False


class OllamaClient:
    """Client Ð´Ð»Ñ Ollama API"""
    
    def __init__(
        self,
        base_url: str = "http://localhost:11434",
        model: str = "llama3.2:3b-instruct",
        timeout: int = 120,
        max_retries: int = 3
    ):
        self.base_url = base_url.rstrip('/')
        self.model = model
        self.timeout = timeout
        self.max_retries = max_retries
        
        # Endpoints
        self.generate_url = f"{self.base_url}/api/generate"
        self.chat_url = f"{self.base_url}/api/chat"
        self.models_url = f"{self.base_url}/api/tags"
    
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        stream: bool = False
    ) -> LLMResponse | Generator[str, None, None]:
        """
        Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ñ–Ñ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ñ– Ð²Ñ–Ð´ LLM
        
        Args:
            prompt: User prompt
            system_prompt: System instructions
            temperature: Sampling temperature (0.0 - 1.0)
            max_tokens: Max tokens to generate
            stream: Whether to stream response
        
        Returns:
            LLMResponse Ð°Ð±Ð¾ Generator Ð´Ð»Ñ streaming
        """
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": stream,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            }
        }
        
        if system_prompt:
            payload["system"] = system_prompt
        
        start_time = time.time()
        
        try:
            if stream:
                return self._generate_stream(payload)
            else:
                return self._generate_complete(payload, start_time)
        
        except Exception as e:
            logger.error(f"LLM generation failed: {e}")
            raise
    
    def _generate_complete(self, payload: Dict, start_time: float) -> LLMResponse:
        """ÐŸÐ¾Ð²Ð½Ð° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ñ–Ñ (non-streaming)"""
        response = self._make_request(self.generate_url, payload)
        
        generation_time = time.time() - start_time
        
        return LLMResponse(
            text=response.get('response', ''),
            model=self.model,
            tokens_generated=response.get('eval_count', 0),
            generation_time=generation_time,
            prompt_tokens=response.get('prompt_eval_count', 0)
        )
    
    def _generate_stream(self, payload: Dict) -> Generator[str, None, None]:
        """Streaming Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ñ–Ñ"""
        response = requests.post(
            self.generate_url,
            json=payload,
            timeout=self.timeout,
            stream=True
        )
        response.raise_for_status()
        
        for line in response.iter_lines():
            if line:
                chunk = json.loads(line)
                if 'response' in chunk:
                    yield chunk['response']
                
                if chunk.get('done', False):
                    break
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> LLMResponse:
        """
        Chat completion (multi-turn conversation)
        
        Args:
            messages: List of {"role": "user/assistant", "content": "..."}
            temperature: Sampling temperature
            max_tokens: Max tokens to generate
        
        Returns:
            LLMResponse
        """
        payload = {
            "model": self.model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            }
        }
        
        start_time = time.time()
        response = self._make_request(self.chat_url, payload)
        generation_time = time.time() - start_time
        
        return LLMResponse(
            text=response['message']['content'],
            model=self.model,
            tokens_generated=response.get('eval_count', 0),
            generation_time=generation_time,
            prompt_tokens=response.get('prompt_eval_count', 0)
        )
    
    def _make_request(self, url: str, payload: Dict) -> Dict:
        """HTTP request Ð· retry logic"""
        for attempt in range(self.max_retries):
            try:
                response = requests.post(
                    url,
                    json=payload,
                    timeout=self.timeout
                )
                response.raise_for_status()
                return response.json()
            
            except requests.exceptions.Timeout:
                logger.warning(f"Request timeout (attempt {attempt + 1}/{self.max_retries})")
                if attempt == self.max_retries - 1:
                    raise
                time.sleep(2 ** attempt)  # Exponential backoff
            
            except requests.exceptions.RequestException as e:
                logger.error(f"Request failed: {e}")
                raise
    
    def list_models(self) -> List[str]:
        """Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹"""
        response = requests.get(self.models_url, timeout=10)
        response.raise_for_status()
        models = response.json().get('models', [])
        return [m['name'] for m in models]
    
    def model_info(self, model_name: Optional[str] = None) -> Dict:
        """Ð†Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–Ñ Ð¿Ñ€Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑŒ"""
        model = model_name or self.model
        response = requests.post(
            f"{self.base_url}/api/show",
            json={"name": model},
            timeout=10
        )
        response.raise_for_status()
        return response.json()
    
    def health_check(self) -> bool:
        """ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚Ñ– Ollama"""
        try:
            response = requests.get(
                f"{self.base_url}/api/version",
                timeout=5
            )
            return response.status_code == 200
        except:
            return False
```

**ÐŸÑ€Ð¸Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ:** HTTP client Ð´Ð»Ñ Ollama Ð· streaming, retry logic, error handling.

---

### 5. **config/settings.py**

```python
"""
ÐÐ°Ð»Ð°ÑˆÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ñƒ
Environment variables, constants, configuration
"""

import os
from pathlib import Path
from typing import Optional
from pydantic import BaseSettings, Field


class Settings(BaseSettings):
    """Ð“Ð¾Ð»Ð¾Ð²Ð½Ñ– Ð½Ð°Ð»Ð°ÑˆÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ñƒ"""
    
    # Project
    PROJECT_NAME: str = "K8s LLM Admin"
    VERSION: str = "0.1.0"
    DEBUG: bool = Field(default=False, env="DEBUG")
    
    # Ollama LLM
    OLLAMA_BASE_URL: str = Field(default="http://localhost:11434", env="OLLAMA_URL")
    OLLAMA_MODEL: str = Field(default="llama3.2:3b-instruct", env="OLLAMA_MODEL")
    OLLAMA_TIMEOUT: int = Field(default=120, env="OLLAMA_TIMEOUT")
    
    # LLM Parameters
    LLM_TEMPERATURE: float = Field(default=0.7, env="LLM_TEMPERATURE")
    LLM_MAX_TOKENS: int = Field(default=2000, env="LLM_MAX_TOKENS")
    LLM_CONTEXT_WINDOW: int = Field(default=8192, env="LLM_CONTEXT_WINDOW")
    
    # Kubernetes
    KUBECONFIG_PATH: Optional[str] = Field(default=None, env="KUBECONFIG")
    DEFAULT_NAMESPACE: str = Field(default="default", env="K8S_NAMESPACE")
    K8S_TIMEOUT: int = Field(default=30, env="K8S_TIMEOUT")
    
    # RAG (Ð´Ð»Ñ ÐšÑ€Ð¾ÐºÑƒ 2)
    VECTOR_DB_PATH: Path = Field(default=Path("./data/vector_db"), env="VECTOR_DB_PATH")
    EMBEDDING_MODEL: str = Field(default="all-MiniLM-L6-v2", env="EMBEDDING_MODEL")
    TOP_K_RESULTS: int = Field(default=5, env="RAG_TOP_K")
    
    # API Server
    API_HOST: str = Field(default="0.0.0.0", env="API_HOST")
    API_PORT: int = Field(default=8000, env="API_PORT")
    API_WORKERS: int = Field(default=1, env="API_WORKERS")
    CORS_ORIGINS: list = Field(default=["*"], env="CORS_ORIGINS")
    
    # Security
    API_KEY: Optional[str] = Field(default=None, env="API_KEY")
    RATE_LIMIT_PER_MINUTE: int = Field(default=30, env="RATE_LIMIT")
    
    # Logging
    LOG_LEVEL: str = Field(default="INFO", env="LOG_LEVEL")
    LOG_FILE: Optional[Path] = Field(default=None, env="LOG_FILE")
    
    # Cache
    ENABLE_CACHE: bool = Field(default=True, env="ENABLE_CACHE")
    CACHE_TTL_SECONDS: int = Field(default=3600, env="CACHE_TTL")
    
    # Monitoring
    ENABLE_METRICS: bool = Field(default=True, env="ENABLE_METRICS")
    METRICS_PORT: int = Field(default=9090, env="METRICS_PORT")
    
    class Config:
        env_file = ".env"
        case_sensitive = True


# Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¸Ð¹ settings instance
settings = Settings()


# Paths
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"
LOGS_DIR = PROJECT_ROOT / "logs"
KNOWLEDGE_BASE_DIR = PROJECT_ROOT / "rag" / "knowledge_base"

# Ð¡Ñ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ñ–Ð¹
for dir_path in [DATA_DIR, LOGS_DIR, KNOWLEDGE_BASE_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)
```

**ÐŸÑ€Ð¸Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ:** Ð¦ÐµÐ½Ñ‚Ñ€Ð°Ð»Ñ–Ð·Ð¾Ð²Ð°Ð½Ð° ÐºÐ¾Ð½Ñ„Ñ–Ð³ÑƒÑ€Ð°Ñ†Ñ–Ñ Ñ‡ÐµÑ€ÐµÐ· environment variables Ð· Pydantic validation.

---

## ðŸš€ ÐšÐ¾Ð¼Ð°Ð½Ð´Ð¸ Ð´Ð»Ñ ÑÑ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ñƒ

### 1. Ð¡Ñ‚Ð²Ð¾Ñ€Ñ–Ñ‚ÑŒ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð½Ð° Ð²Ð°ÑˆÐ¾Ð¼Ñƒ ÐºÐ¾Ð¼Ð¿'ÑŽÑ‚ÐµÑ€Ñ–:

```bash
# Ð¡Ñ‚Ð²Ð¾Ñ€Ð¸Ñ‚Ð¸ Ð¿Ñ€Ð¾ÐµÐºÑ‚
mkdir -p k8s-llm-admin
cd k8s-llm-admin

# Git init
git init
echo "# K8s LLM Admin Assistant" > README.md

# Ð¡Ñ‚Ð²Ð¾Ñ€Ð¸Ñ‚Ð¸ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ñ–Ð¹
mkdir -p config prompts llm k8s rag api/routes api/models api/middleware utils tests scripts docs examples

# Ð¡Ñ‚Ð²Ð¾Ñ€Ð¸Ñ‚Ð¸ __init__.py Ñ„Ð°Ð¹Ð»Ð¸
find . -type d -exec touch {}/__init__.py \;

# .gitignore
cat > .gitignore << 'EOF'
__pycache__/
*.py[cod]
*$py.class
.env
.venv/
venv/
*.log
.DS_Store
data/
logs/
*.swp
.idea/
.vscode/
EOF
```

### 2. requirements.txt:

```txt
# LLM
requests==2.31.0

# API
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
python-multipart==0.0.6

# Kubernetes
kubernetes==28.1.0
pyyaml==6.0.1

# RAG (ÐšÑ€Ð¾Ðº 2)
chromadb==0.4.18
sentence-transformers==2.2.2
langchain==0.0.340

# Templates
jinja2==3.1.2

# Utilities
python-dotenv==1.0.0
aiohttp==3.9.1
httpx==0.25.2

# Logging & Monitoring
loguru==0.7.2
prometheus-client==0.19.0

# Testing
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0
httpx==0.25.2

# Dev tools
black==23.11.0
flake8==6.1.0
mypy==1.7.1
```

### 3. .env.example:

```bash
# LLM Configuration
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b-instruct
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2000

# Kubernetes
KUBECONFIG=/path/to/kubeconfig
K8S_NAMESPACE=default

# API
API_HOST=0.0.0.0
API_PORT=8000
DEBUG=false

# Security
API_KEY=your-secret-key-here
RATE_LIMIT=30

# Logging
LOG_LEVEL=INFO
LOG_FILE=./logs/app.log

# Cache
ENABLE_CACHE=true
CACHE_TTL=3600
```

---

## ðŸ“¦ Ð’ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ Ð½Ð° Jetson

### Git push Ñ– pull:

```bash
# ÐÐ° Ð²Ð°ÑˆÐ¾Ð¼Ñƒ ÐºÐ¾Ð¼Ð¿'ÑŽÑ‚ÐµÑ€Ñ–
git add .
git commit -m "Initial project structure"
git remote add origin https://github.com/your-username/k8s-llm-admin.git
git push -u origin main

# ÐÐ° Jetson
cd ~
git clone https://github.com/your-username/k8s-llm-admin.git
cd k8s-llm-admin

# Ð’Ñ–Ñ€Ñ‚ÑƒÐ°Ð»ÑŒÐ½Ðµ ÑÐµÑ€ÐµÐ´Ð¾Ð²Ð¸Ñ‰Ðµ
python3 -m venv venv
source venv/bin/activate

# Ð—Ð°Ð»ÐµÐ¶Ð½Ð¾ÑÑ‚Ñ–
pip install --upgrade pip
pip install -r requirements.txt

# .env Ñ„Ð°Ð¹Ð»
cp .env.example .env
nano .env  # Ð’Ñ–Ð´Ñ€ÐµÐ´Ð°Ð³ÑƒÐ²Ð°Ñ‚Ð¸ Ð½Ð°Ð»Ð°ÑˆÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ

# Ð¢ÐµÑÑ‚
python -m pytest tests/
```

---

## âœ… Checklist ÑÑ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ñƒ

```
[ ] Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ñ–Ð¹ ÑÑ‚Ð²Ð¾Ñ€ÐµÐ½Ð°
[ ] __init__.py Ñ„Ð°Ð¹Ð»Ð¸ Ð² ÐºÐ¾Ð¶Ð½Ñ–Ð¹ Ð¿Ð°Ð¿Ñ†Ñ–
[ ] .gitignore Ð½Ð°Ð»Ð°ÑˆÑ‚Ð¾Ð²Ð°Ð½Ð¸Ð¹
[ ] requirements.txt ÑÑ‚Ð²Ð¾Ñ€ÐµÐ½Ð¸Ð¹
[ ] .env.example ÑÑ‚Ð²Ð¾Ñ€ÐµÐ½Ð¸Ð¹
[ ] Git repository Ñ–Ð½Ñ–Ñ†Ñ–Ð°Ð»Ñ–Ð·Ð¾Ð²Ð°Ð½Ð¸Ð¹
[ ] prompts/system_prompts.py ÑÐºÐ¾Ð¿Ñ–Ð¹Ð¾Ð²Ð°Ð½Ð¸Ð¹ Ð· artifact
[ ] prompts/templates.py ÑÑ‚Ð²Ð¾Ñ€ÐµÐ½Ð¸Ð¹
[ ] prompts/validators.py ÑÑ‚Ð²Ð¾Ñ€ÐµÐ½Ð¸Ð¹
[ ] llm/ollama_client.py ÑÑ‚Ð²Ð¾Ñ€ÐµÐ½Ð¸Ð¹
[ ] config/settings.py ÑÑ‚Ð²Ð¾Ñ€ÐµÐ½Ð¸Ð¹
[ ] README.md Ð· Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ñ–Ñ”ÑŽ
[ ] Git push Ð½Ð° GitHub
[ ] Git clone Ð½Ð° Jetson
[ ] Virtual environment Ð½Ð° Jetson
[ ] Dependencies Ð²ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ñ–
[ ] Ollama Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ð¸Ð¹ Ð½Ð° Jetson
[ ] Ð‘Ð°Ð·Ð¾Ð²Ð¸Ð¹ Ñ‚ÐµÑÑ‚ Ð¿Ñ€Ð¾Ð¹ÑˆÐ¾Ð²
```

---